{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c19a709",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f85ab668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random, math, re, pandas as pd\n",
    "from typing import List\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f6ccfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1383c1010>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39c15b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu as device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} as device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dbb7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./gutenberg.txt\", encoding=\"utf-8\") as f:\n",
    "    sentences_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17179d",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "479742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split() if text else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7cfa3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5298 sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for line in sentences_raw:\n",
    "    tokens = preprocess_text(line)\n",
    "    if tokens:\n",
    "        sentences.append(tokens)\n",
    "print(len(sentences), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffd2d1",
   "metadata": {},
   "source": [
    "### Subsampling + Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8abdf",
   "metadata": {},
   "source": [
    "- Applied subsampling to the corpus to reduce the influence of very frequent words. For each word, it computes a keep probability based on its frequency, then randomly drops words according to that probability to create subsampled_sentences -> reduce influence of words like (the, is, was, of, and ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e41187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct tokens: 5014\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "total_tokens = 0\n",
    "for sent in sentences:\n",
    "    counter.update(sent)\n",
    "    total_tokens += len(sent)\n",
    "\n",
    "freq = {w: c / total_tokens for w, c in counter.items()}\n",
    "word_counts = OrderedDict(\n",
    "    sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    ")\n",
    "print(\"Distinct tokens:\", len(word_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "952e4be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling: 61920\n",
      "After subsampling: 43565\n"
     ]
    }
   ],
   "source": [
    "t = 1e-3\n",
    "\n",
    "def keep_prob(word):\n",
    "    f = freq[word]\n",
    "    return math.sqrt(t / f) + (t / f)\n",
    "\n",
    "subsampled_sentences = []\n",
    "for sent in sentences:\n",
    "    new_sent = []\n",
    "    for w in sent:\n",
    "        if w not in freq:\n",
    "            new_sent.append(w)\n",
    "            continue\n",
    "        if random.random() < keep_prob(w):\n",
    "            new_sent.append(w)\n",
    "    if new_sent:\n",
    "        subsampled_sentences.append(new_sent)\n",
    "\n",
    "print(\"Before subsampling:\", sum(len(s) for s in sentences))\n",
    "print(\"After subsampling:\", sum(len(s) for s in subsampled_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1db72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which words are removed\n",
    "\n",
    "counter_before = Counter()\n",
    "for sent in sentences:\n",
    "    counter_before.update(sent)\n",
    "\n",
    "counter_after = Counter()\n",
    "for sent in subsampled_sentences:\n",
    "    counter_after.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33f8fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the             before= 3633, after=  530, dropped= 3103, ratio=85.41%\n",
      "of              before= 2629, after=  481, dropped= 2148, ratio=81.70%\n",
      "to              before= 2197, after=  443, dropped= 1754, ratio=79.84%\n",
      "and             before= 2164, after=  468, dropped= 1696, ratio=78.37%\n",
      "that            before=  978, after=  288, dropped=  690, ratio=70.55%\n",
      "a               before= 1014, after=  305, dropped=  709, ratio=69.92%\n",
      "in              before= 1055, after=  320, dropped=  735, ratio=69.67%\n",
      "it              before=  893, after=  308, dropped=  585, ratio=65.51%\n",
      "is              before=  843, after=  304, dropped=  539, ratio=63.94%\n",
      "be              before=  714, after=  266, dropped=  448, ratio=62.75%\n",
      "or              before=  626, after=  250, dropped=  376, ratio=60.06%\n",
      "as              before=  634, after=  257, dropped=  377, ratio=59.46%\n",
      "by              before=  570, after=  232, dropped=  338, ratio=59.30%\n",
      "his             before=  618, after=  256, dropped=  362, ratio=58.58%\n",
      "for             before=  589, after=  248, dropped=  341, ratio=57.89%\n",
      "he              before=  559, after=  253, dropped=  306, ratio=54.74%\n",
      "any             before=  484, after=  222, dropped=  262, ratio=54.13%\n",
      "their           before=  466, after=  219, dropped=  247, ratio=53.00%\n",
      "they            before=  475, after=  239, dropped=  236, ratio=49.68%\n",
      "not             before=  433, after=  221, dropped=  212, ratio=48.96%\n",
      "which           before=  464, after=  238, dropped=  226, ratio=48.71%\n",
      "power           before=  416, after=  220, dropped=  196, ratio=47.12%\n",
      "but             before=  421, after=  223, dropped=  198, ratio=47.03%\n",
      "have            before=  404, after=  214, dropped=  190, ratio=47.03%\n",
      "all             before=  380, after=  208, dropped=  172, ratio=45.26%\n",
      "this            before=  372, after=  207, dropped=  165, ratio=44.35%\n",
      "are             before=  366, after=  211, dropped=  155, ratio=42.35%\n",
      "them            before=  322, after=  202, dropped=  120, ratio=37.27%\n",
      "one             before=  318, after=  202, dropped=  116, ratio=36.48%\n",
      "no              before=  326, after=  208, dropped=  118, ratio=36.20%\n"
     ]
    }
   ],
   "source": [
    "removed_stats = []\n",
    "\n",
    "for w, c_before in counter_before.items():\n",
    "    c_after = counter_after.get(w, 0)\n",
    "    if c_before == 0:\n",
    "        continue\n",
    "    drop = c_before - c_after\n",
    "    drop_ratio = drop / c_before  \n",
    "\n",
    "    removed_stats.append((w, c_before, c_after, drop, drop_ratio))\n",
    "\n",
    "removed_stats.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "# top 30\n",
    "for w, cb, ca, drop, ratio in removed_stats[:30]:\n",
    "    print(f\"{w:15s} before={cb:5d}, after={ca:5d}, dropped={drop:5d}, ratio={ratio:6.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b6d2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_counts: OrderedDict, # vocabular is based on word counts\n",
    "        min_freq: int = 1, # min times a word must appear in corpus (rare words might not be worth considering)\n",
    "        max_size: int = None, # we can limit the amount of words as well \n",
    "        specials: List[str] = None, # any other special tokens we may want to add, like padding tokens\n",
    "        unk_token: str = \"<unk>\" # reserved token for when we run into words not in the vocabulary\n",
    "    ):\n",
    "        self.word_counts = word_counts\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.unk_token = unk_token\n",
    "        self.specials = list(specials) if specials else []\n",
    "\n",
    "        if self.unk_token not in self.specials:\n",
    "            self.specials.insert(0, self.unk_token) # unknown token should always be included\n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "        self._prepare_vocab()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "\n",
    "    def __contains__(self, value):\n",
    "        return value in self.idx2token\n",
    "\n",
    "\n",
    "    def _prepare_vocab(self):\n",
    "        \"\"\"Processes input OrderedDict: Filters based on min_freq & adds special tokens.\"\"\"\n",
    "        vocab_list = self.specials.copy()  # Copy specials to avoid modifying original list\n",
    "\n",
    "        # filter words based on min_freq and add to vocab\n",
    "        filtered_words = [\n",
    "            word\n",
    "            for word, freq in self.word_counts.items()\n",
    "            if freq >= self.min_freq and word not in self.specials\n",
    "        ]\n",
    "\n",
    "        # enforcing max vocab size constraint\n",
    "        if self.max_size is not None:\n",
    "            n_to_keep = self.max_size - len(self.specials) # special tokens take up spaces\n",
    "            filtered_words = filtered_words[:n_to_keep]\n",
    "\n",
    "        # creating final vocab list\n",
    "        vocab_list.extend(word for word in filtered_words)\n",
    "\n",
    "        # create look up tables\n",
    "        self.idx2token = vocab_list\n",
    "        self.token2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "\n",
    "    def get_token(self, idx: int) -> str:\n",
    "        \"\"\"Returns the token corresponding to an index. Raises error if index is out of range.\"\"\"\n",
    "        if 0 <= idx < len(self.idx2token):\n",
    "            return self.idx2token[idx]\n",
    "        raise IndexError(f\"Index {idx} is out of range for vocabulary size {len(self.idx2token)}\")\n",
    "\n",
    "\n",
    "    def get_index(self, token: str) -> int:\n",
    "        \"\"\"Returns the index corresponding to a token. Defaults to unk_token if missing.\"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])  # return unk_token index if word is not in vocab\n",
    "\n",
    "\n",
    "    def get_tokens(self, indices: List[int]) -> List[str]:\n",
    "        \"\"\"Converts a list of indices into a list of tokens.\"\"\"\n",
    "        return [self.get_token(idx) for idx in indices]\n",
    "\n",
    "\n",
    "    def get_indices(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Converts a list of tokens into a list of indices.\"\"\"\n",
    "        return [self.get_index(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ef434fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences: List[List[str]], context_length: int, pad_token: str = \"<pad>\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Pads each sentence to fit the context window length with the literal string \"<pad>\".\n",
    "    \n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the target token.\n",
    "\n",
    "    Returns:\n",
    "        A list of padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence = [pad_token] * context_length + sentence + [pad_token] * context_length\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    \n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fda8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_sub = Counter()\n",
    "for sent in subsampled_sentences:\n",
    "    counter_sub.update(sent)\n",
    "\n",
    "word_counts_sub = OrderedDict(\n",
    "    sorted(counter_sub.items(), key=lambda x: x[1], reverse=True)\n",
    ")\n",
    "\n",
    "MIN_FREQ = 5\n",
    "vocab = Vocab(\n",
    "    word_counts=word_counts_sub,\n",
    "    min_freq=MIN_FREQ,\n",
    "    max_size=None,\n",
    "    specials=[\"<pad>\"],\n",
    "    unk_token=\"<unk>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7cee263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 1,183\n"
     ]
    }
   ],
   "source": [
    "# creating a vocabulary\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf1cd0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 corresponds to `<unk>`\n",
      "Index 1 corresponds to `<pad>`\n",
      "Index 5 corresponds to `to`\n",
      "Index 100 corresponds to `up`\n"
     ]
    }
   ],
   "source": [
    "for idx in [0, 1, 5, 100]:\n",
    "    print(f\"Index {idx} corresponds to `{vocab.get_token(idx)}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1155a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude <pad> and <unk> from both target and context.\n",
    "PAD_IDX = vocab.get_index(\"<pad>\") \n",
    "UNK_IDX = vocab.get_index(\"<unk>\")\n",
    "\n",
    "def generate_skipgram(sentences, context_length, vocab):\n",
    "    targets = []\n",
    "    contexts = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        enc_sentence = vocab.get_indices(sentence)\n",
    "\n",
    "        for target_idx in range(context_length, len(enc_sentence) - context_length):\n",
    "            target = enc_sentence[target_idx]\n",
    "            if target in (PAD_IDX, UNK_IDX):\n",
    "                continue\n",
    "\n",
    "            for offset in range(-context_length, context_length + 1):\n",
    "                if offset == 0:\n",
    "                    continue\n",
    "                context_idx = target_idx + offset\n",
    "                context = enc_sentence[context_idx]\n",
    "                if context in (PAD_IDX, UNK_IDX):\n",
    "                    continue\n",
    "\n",
    "                targets.append(target)\n",
    "                contexts.append(context)\n",
    "\n",
    "    return torch.tensor(targets), torch.tensor(contexts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fd5ea",
   "metadata": {},
   "source": [
    "## Dataset / DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61efd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, targets, contexts):\n",
    "        self.targets = targets\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.targets[idx], self.contexts[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ce7d9",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46c858b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, target_indices):\n",
    "        # target_indices: [batch_size]\n",
    "        emb = self.emb(target_indices)         # [batch_size, embedding_dim]\n",
    "        logits = self.out(emb)                # [batch_size, vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a2b34",
   "metadata": {},
   "source": [
    "## Model Train / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac050c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_from_embeddings(word, embeddings, vocab, top_k=10):\n",
    "    if word not in vocab.token2idx:\n",
    "        return []  \n",
    "\n",
    "    idx = vocab.get_index(word)\n",
    "    vec = embeddings[idx]  # [embedding_dim]\n",
    "\n",
    "    sims = F.cosine_similarity(vec.unsqueeze(0), embeddings)  # [vocab_size]\n",
    "    topk = torch.topk(sims, top_k + 1) \n",
    "\n",
    "    results = []\n",
    "    for score, i in zip(topk.values.tolist(), topk.indices.tolist()):\n",
    "        token = vocab.get_token(i)\n",
    "        if token == word:\n",
    "            continue\n",
    "        results.append((token, score))\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    return results  # [(neighbor, score), ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31efec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct tokens: 5014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 3633),\n",
       " ('of', 2629),\n",
       " ('to', 2197),\n",
       " ('and', 2164),\n",
       " ('in', 1055),\n",
       " ('a', 1014),\n",
       " ('that', 978),\n",
       " ('it', 893),\n",
       " ('is', 843),\n",
       " ('be', 714)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "counter = Counter()\n",
    "for sent in sentences:         \n",
    "    counter.update(sent)\n",
    "\n",
    "word_counts = OrderedDict(\n",
    "    sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    ")\n",
    "\n",
    "print(\"Number of distinct tokens:\", len(word_counts))\n",
    "list(word_counts.items())[:10]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7249e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_list       = [2, 4]\n",
    "embedding_list    = [50, 100]\n",
    "min_freq_list     = [5]          \n",
    "batch_size_list   = [64]\n",
    "n_epochs_list     = [2]\n",
    "\n",
    "# Words to check\n",
    "probe_words = [\"freedom\", \"law\", \"power\"]\n",
    "TOP_K = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f55f973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Config 1 ===\n",
      "WINDOW=2, EMB_DIM=50, MIN_FREQ=5, BATCH=64, EPOCHS=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 - Loss: 5.9218\n",
      "  Epoch 2/2 - Loss: 5.2842\n",
      "\n",
      "=== Config 2 ===\n",
      "WINDOW=2, EMB_DIM=100, MIN_FREQ=5, BATCH=64, EPOCHS=2\n",
      "  Epoch 1/2 - Loss: 5.7862\n",
      "  Epoch 2/2 - Loss: 5.1794\n",
      "\n",
      "=== Config 3 ===\n",
      "WINDOW=4, EMB_DIM=50, MIN_FREQ=5, BATCH=64, EPOCHS=2\n",
      "  Epoch 1/2 - Loss: 5.9686\n",
      "  Epoch 2/2 - Loss: 5.4223\n",
      "\n",
      "=== Config 4 ===\n",
      "WINDOW=4, EMB_DIM=100, MIN_FREQ=5, BATCH=64, EPOCHS=2\n",
      "  Epoch 1/2 - Loss: 5.8641\n",
      "  Epoch 2/2 - Loss: 5.3553\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "\n",
    "config_id = 0\n",
    "\n",
    "for CONTEXT_WINDOW, EMBEDDING_SIZE, MIN_FREQ, BATCH_SIZE, N_EPOCHS in product(\n",
    "    window_list, embedding_list, min_freq_list, batch_size_list, n_epochs_list\n",
    "):\n",
    "    config_id += 1\n",
    "    print(f\"\\n=== Config {config_id} ===\")\n",
    "    print(f\"WINDOW={CONTEXT_WINDOW}, EMB_DIM={EMBEDDING_SIZE}, MIN_FREQ={MIN_FREQ}, \"\n",
    "          f\"BATCH={BATCH_SIZE}, EPOCHS={N_EPOCHS}\")\n",
    "    \n",
    "    # skip-gram pair\n",
    "    targets, contexts = generate_skipgram(sentences=sentences, context_length=CONTEXT_WINDOW,vocab=vocab)\n",
    "\n",
    "    dataset = SkipGramDataset(targets, contexts)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\n",
    "    \n",
    "    # model define\n",
    "    model = SkipGramModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=EMBEDDING_SIZE,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # model train\n",
    "    model.train()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        for batch_targets, batch_contexts in dataloader:\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            batch_contexts = batch_contexts.to(device)\n",
    "\n",
    "            logits = model(batch_targets)\n",
    "            loss = criterion(logits, batch_contexts)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch_targets.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataset)\n",
    "        print(f\"  Epoch {epoch+1}/{N_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # most_similar results based on trained embeddings\n",
    "    embeddings = model.emb.weight.detach().cpu()\n",
    "\n",
    "    for word in probe_words:\n",
    "        neighbors = most_similar_from_embeddings(word, embeddings, vocab, top_k=TOP_K)\n",
    "\n",
    "        if not neighbors:\n",
    "            results.append({\n",
    "                \"config_id\": config_id,\n",
    "                \"window\": CONTEXT_WINDOW,\n",
    "                \"embedding_dim\": EMBEDDING_SIZE,\n",
    "                \"min_freq\": MIN_FREQ,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"epochs\": N_EPOCHS,\n",
    "                \"word\": word,\n",
    "                \"neighbor_rank\": None,\n",
    "                \"neighbor\": None,\n",
    "                \"similarity\": None,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for rank, (nbr, score) in enumerate(neighbors, start=1):\n",
    "            results.append({\n",
    "                \"config_id\": config_id,\n",
    "                \"window\": CONTEXT_WINDOW,\n",
    "                \"embedding_dim\": EMBEDDING_SIZE,\n",
    "                \"min_freq\": MIN_FREQ,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"epochs\": N_EPOCHS,\n",
    "                \"word\": word,\n",
    "                \"neighbor_rank\": rank,\n",
    "                \"neighbor\": nbr,\n",
    "                \"similarity\": score,\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56d2fa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_id</th>\n",
       "      <th>window</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>word</th>\n",
       "      <th>neighbor_rank</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>1</td>\n",
       "      <td>your</td>\n",
       "      <td>0.472956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "      <td>governing</td>\n",
       "      <td>0.472150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>3</td>\n",
       "      <td>cease</td>\n",
       "      <td>0.452665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>4</td>\n",
       "      <td>together</td>\n",
       "      <td>0.413366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>5</td>\n",
       "      <td>assembling</td>\n",
       "      <td>0.410174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>1</td>\n",
       "      <td>damages</td>\n",
       "      <td>0.499466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>2</td>\n",
       "      <td>laws</td>\n",
       "      <td>0.493520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>3</td>\n",
       "      <td>then</td>\n",
       "      <td>0.473101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>4</td>\n",
       "      <td>atque</td>\n",
       "      <td>0.441182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>5</td>\n",
       "      <td>remain</td>\n",
       "      <td>0.440887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>1</td>\n",
       "      <td>redress</td>\n",
       "      <td>0.460568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>2</td>\n",
       "      <td>slavery</td>\n",
       "      <td>0.421594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>3</td>\n",
       "      <td>fault</td>\n",
       "      <td>0.419357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>0.407635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>5</td>\n",
       "      <td>earth</td>\n",
       "      <td>0.395677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>1</td>\n",
       "      <td>become</td>\n",
       "      <td>0.328429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "      <td>declared</td>\n",
       "      <td>0.319667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>3</td>\n",
       "      <td>monarchical</td>\n",
       "      <td>0.309983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>4</td>\n",
       "      <td>way</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>5</td>\n",
       "      <td>god</td>\n",
       "      <td>0.298144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>1</td>\n",
       "      <td>whereof</td>\n",
       "      <td>0.368661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>2</td>\n",
       "      <td>safe</td>\n",
       "      <td>0.331873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>3</td>\n",
       "      <td>access</td>\n",
       "      <td>0.320624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>4</td>\n",
       "      <td>destruction</td>\n",
       "      <td>0.303421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>5</td>\n",
       "      <td>wish</td>\n",
       "      <td>0.300633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>1</td>\n",
       "      <td>world</td>\n",
       "      <td>0.377886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>2</td>\n",
       "      <td>judges</td>\n",
       "      <td>0.335934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>3</td>\n",
       "      <td>properly</td>\n",
       "      <td>0.315025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>4</td>\n",
       "      <td>successors</td>\n",
       "      <td>0.314375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>5</td>\n",
       "      <td>join</td>\n",
       "      <td>0.307347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    config_id  window  embedding_dim  min_freq  batch_size  epochs     word  \\\n",
       "0           1       2             50         5          64       2  freedom   \n",
       "1           1       2             50         5          64       2  freedom   \n",
       "2           1       2             50         5          64       2  freedom   \n",
       "3           1       2             50         5          64       2  freedom   \n",
       "4           1       2             50         5          64       2  freedom   \n",
       "5           1       2             50         5          64       2      law   \n",
       "6           1       2             50         5          64       2      law   \n",
       "7           1       2             50         5          64       2      law   \n",
       "8           1       2             50         5          64       2      law   \n",
       "9           1       2             50         5          64       2      law   \n",
       "10          1       2             50         5          64       2    power   \n",
       "11          1       2             50         5          64       2    power   \n",
       "12          1       2             50         5          64       2    power   \n",
       "13          1       2             50         5          64       2    power   \n",
       "14          1       2             50         5          64       2    power   \n",
       "15          2       2            100         5          64       2  freedom   \n",
       "16          2       2            100         5          64       2  freedom   \n",
       "17          2       2            100         5          64       2  freedom   \n",
       "18          2       2            100         5          64       2  freedom   \n",
       "19          2       2            100         5          64       2  freedom   \n",
       "20          2       2            100         5          64       2      law   \n",
       "21          2       2            100         5          64       2      law   \n",
       "22          2       2            100         5          64       2      law   \n",
       "23          2       2            100         5          64       2      law   \n",
       "24          2       2            100         5          64       2      law   \n",
       "25          2       2            100         5          64       2    power   \n",
       "26          2       2            100         5          64       2    power   \n",
       "27          2       2            100         5          64       2    power   \n",
       "28          2       2            100         5          64       2    power   \n",
       "29          2       2            100         5          64       2    power   \n",
       "\n",
       "    neighbor_rank     neighbor  similarity  \n",
       "0               1         your    0.472956  \n",
       "1               2    governing    0.472150  \n",
       "2               3        cease    0.452665  \n",
       "3               4     together    0.413366  \n",
       "4               5   assembling    0.410174  \n",
       "5               1      damages    0.499466  \n",
       "6               2         laws    0.493520  \n",
       "7               3         then    0.473101  \n",
       "8               4        atque    0.441182  \n",
       "9               5       remain    0.440887  \n",
       "10              1      redress    0.460568  \n",
       "11              2      slavery    0.421594  \n",
       "12              3        fault    0.419357  \n",
       "13              4         call    0.407635  \n",
       "14              5        earth    0.395677  \n",
       "15              1       become    0.328429  \n",
       "16              2     declared    0.319667  \n",
       "17              3  monarchical    0.309983  \n",
       "18              4          way    0.307100  \n",
       "19              5          god    0.298144  \n",
       "20              1      whereof    0.368661  \n",
       "21              2         safe    0.331873  \n",
       "22              3       access    0.320624  \n",
       "23              4  destruction    0.303421  \n",
       "24              5         wish    0.300633  \n",
       "25              1        world    0.377886  \n",
       "26              2       judges    0.335934  \n",
       "27              3     properly    0.315025  \n",
       "28              4   successors    0.314375  \n",
       "29              5         join    0.307347  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# window_size = 2, embedding_dim = [50, 100]\n",
    "df_results[df_results['window'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33883b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config_id</th>\n",
       "      <th>window</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>word</th>\n",
       "      <th>neighbor_rank</th>\n",
       "      <th>neighbor</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>1</td>\n",
       "      <td>injured</td>\n",
       "      <td>0.422753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "      <td>act</td>\n",
       "      <td>0.415491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>3</td>\n",
       "      <td>nations</td>\n",
       "      <td>0.414656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>4</td>\n",
       "      <td>enter</td>\n",
       "      <td>0.412241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>5</td>\n",
       "      <td>indeed</td>\n",
       "      <td>0.399403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>1</td>\n",
       "      <td>make</td>\n",
       "      <td>0.497718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>2</td>\n",
       "      <td>people</td>\n",
       "      <td>0.460036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>3</td>\n",
       "      <td>destroy</td>\n",
       "      <td>0.444769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>4</td>\n",
       "      <td>express</td>\n",
       "      <td>0.423579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>5</td>\n",
       "      <td>find</td>\n",
       "      <td>0.421653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>1</td>\n",
       "      <td>possession</td>\n",
       "      <td>0.475363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>2</td>\n",
       "      <td>noxious</td>\n",
       "      <td>0.443718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>3</td>\n",
       "      <td>thing</td>\n",
       "      <td>0.428560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>4</td>\n",
       "      <td>offences</td>\n",
       "      <td>0.423275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>5</td>\n",
       "      <td>speaking</td>\n",
       "      <td>0.417502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>1</td>\n",
       "      <td>down</td>\n",
       "      <td>0.386702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>2</td>\n",
       "      <td>come</td>\n",
       "      <td>0.354511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>3</td>\n",
       "      <td>born</td>\n",
       "      <td>0.339952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>4</td>\n",
       "      <td>give</td>\n",
       "      <td>0.320539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>freedom</td>\n",
       "      <td>5</td>\n",
       "      <td>owing</td>\n",
       "      <td>0.301106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>1</td>\n",
       "      <td>health</td>\n",
       "      <td>0.302553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>2</td>\n",
       "      <td>god</td>\n",
       "      <td>0.293332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>3</td>\n",
       "      <td>waste</td>\n",
       "      <td>0.286173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>4</td>\n",
       "      <td>tie</td>\n",
       "      <td>0.280346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>law</td>\n",
       "      <td>5</td>\n",
       "      <td>exercise</td>\n",
       "      <td>0.279584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>1</td>\n",
       "      <td>words</td>\n",
       "      <td>0.326941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>2</td>\n",
       "      <td>first</td>\n",
       "      <td>0.322205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>3</td>\n",
       "      <td>settled</td>\n",
       "      <td>0.313753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>4</td>\n",
       "      <td>best</td>\n",
       "      <td>0.300597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>power</td>\n",
       "      <td>5</td>\n",
       "      <td>miscarriages</td>\n",
       "      <td>0.294047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    config_id  window  embedding_dim  min_freq  batch_size  epochs     word  \\\n",
       "30          3       4             50         5          64       2  freedom   \n",
       "31          3       4             50         5          64       2  freedom   \n",
       "32          3       4             50         5          64       2  freedom   \n",
       "33          3       4             50         5          64       2  freedom   \n",
       "34          3       4             50         5          64       2  freedom   \n",
       "35          3       4             50         5          64       2      law   \n",
       "36          3       4             50         5          64       2      law   \n",
       "37          3       4             50         5          64       2      law   \n",
       "38          3       4             50         5          64       2      law   \n",
       "39          3       4             50         5          64       2      law   \n",
       "40          3       4             50         5          64       2    power   \n",
       "41          3       4             50         5          64       2    power   \n",
       "42          3       4             50         5          64       2    power   \n",
       "43          3       4             50         5          64       2    power   \n",
       "44          3       4             50         5          64       2    power   \n",
       "45          4       4            100         5          64       2  freedom   \n",
       "46          4       4            100         5          64       2  freedom   \n",
       "47          4       4            100         5          64       2  freedom   \n",
       "48          4       4            100         5          64       2  freedom   \n",
       "49          4       4            100         5          64       2  freedom   \n",
       "50          4       4            100         5          64       2      law   \n",
       "51          4       4            100         5          64       2      law   \n",
       "52          4       4            100         5          64       2      law   \n",
       "53          4       4            100         5          64       2      law   \n",
       "54          4       4            100         5          64       2      law   \n",
       "55          4       4            100         5          64       2    power   \n",
       "56          4       4            100         5          64       2    power   \n",
       "57          4       4            100         5          64       2    power   \n",
       "58          4       4            100         5          64       2    power   \n",
       "59          4       4            100         5          64       2    power   \n",
       "\n",
       "    neighbor_rank      neighbor  similarity  \n",
       "30              1       injured    0.422753  \n",
       "31              2           act    0.415491  \n",
       "32              3       nations    0.414656  \n",
       "33              4         enter    0.412241  \n",
       "34              5        indeed    0.399403  \n",
       "35              1          make    0.497718  \n",
       "36              2        people    0.460036  \n",
       "37              3       destroy    0.444769  \n",
       "38              4       express    0.423579  \n",
       "39              5          find    0.421653  \n",
       "40              1    possession    0.475363  \n",
       "41              2       noxious    0.443718  \n",
       "42              3         thing    0.428560  \n",
       "43              4      offences    0.423275  \n",
       "44              5      speaking    0.417502  \n",
       "45              1          down    0.386702  \n",
       "46              2          come    0.354511  \n",
       "47              3          born    0.339952  \n",
       "48              4          give    0.320539  \n",
       "49              5         owing    0.301106  \n",
       "50              1        health    0.302553  \n",
       "51              2           god    0.293332  \n",
       "52              3         waste    0.286173  \n",
       "53              4           tie    0.280346  \n",
       "54              5      exercise    0.279584  \n",
       "55              1         words    0.326941  \n",
       "56              2         first    0.322205  \n",
       "57              3       settled    0.313753  \n",
       "58              4          best    0.300597  \n",
       "59              5  miscarriages    0.294047  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# window_size = 4, embedding_dim = [50, 100]\n",
    "df_results[df_results['window'] == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49f304",
   "metadata": {},
   "source": [
    "- I tried different context window sizes (2,4) and two embedding sizes (50, 100). \n",
    "- Qualitatively, the Skip-gram model with a larger context window (4) and 50-dimensional embeddings produced the most coherent neighborhoods for words like freedom, law, and power (e.g., law -> destroy, express; power -> possession, offences). \n",
    "- This suggests that a slightly wider context helps the model capture the political and legal themes present in our corpus better than a smaller window size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59176689",
   "metadata": {},
   "source": [
    "## Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "657507e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[random (untrained)] neighbors of 'freedom':\n",
      "   1. tax             0.2836\n",
      "   2. conquering      0.2804\n",
      "   3. neither         0.2647\n",
      "   4. designed        0.2572\n",
      "   5. f               0.2557\n",
      "\n",
      "[random (untrained)] neighbors of 'law':\n",
      "   1. her             0.3707\n",
      "   2. damages         0.2936\n",
      "   3. besides         0.2889\n",
      "   4. punishment      0.2865\n",
      "   5. work            0.2798\n",
      "\n",
      "[random (untrained)] neighbors of 'power':\n",
      "   1. commonwealths   0.3580\n",
      "   2. proved          0.3338\n",
      "   3. prove           0.3295\n",
      "   4. arbitrary       0.2966\n",
      "   5. thinks          0.2774\n"
     ]
    }
   ],
   "source": [
    "def show_neighbors(word, model, vocab, top_k=5, title=\"\"):\n",
    "    print(f\"\\n[{title}] neighbors of '{word}':\")\n",
    "    \n",
    "    if word not in vocab.token2idx:\n",
    "        print(f\"  '{word}' not in vocabulary\")\n",
    "        return\n",
    "\n",
    "    embeddings = model.emb.weight.detach().cpu()\n",
    "    neighbors = most_similar_from_embeddings(word, embeddings, vocab, top_k=top_k)\n",
    "\n",
    "    if not neighbors:\n",
    "        print(\"  (no neighbors found)\")\n",
    "        return\n",
    "\n",
    "    for rank, (nbr, score) in enumerate(neighbors, start=1):\n",
    "        print(f\"  {rank:2d}. {nbr:15s} {score:.4f}\")\n",
    "\n",
    "# Untrained model\n",
    "random_model = SkipGramModel(len(vocab), EMBEDDING_SIZE).to(device)  \n",
    "\n",
    "for w in [\"freedom\", \"law\", \"power\"]:\n",
    "    show_neighbors(w, random_model, vocab, top_k=5, title=\"random (untrained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54bc7a",
   "metadata": {},
   "source": [
    "- For the untrained random model, the neighbors of freedom and law are essentially arbitrary content words, with no clear semantic or legal/political theme. A few words like damages, punishment, or commonwealths happen to be loosely related, but overall there is no consistent semantic pattern: the lists mix unrelated verbs, pronouns, and random nouns with similar cosine scores.\n",
    "- In contrast, trained Skip-gram model produces neighbors that are clearly related to the underlying concepts, showing that the learned embeddings capture meaningful structure that is not present in random embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d95d3",
   "metadata": {},
   "source": [
    "### CBOW vs Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab2b10",
   "metadata": {},
   "source": [
    "- CBOW and Skip-gram architectures differ mainly in what they take as input and how they aggregate embeddings.\n",
    "\n",
    "1. Skip-gram (SkipGramModel):\n",
    "    - Input: a single target (center) word index per example, shape [batch_size].\n",
    "    - Forward: emb = self.emb(target_indices) gives [batch_size, embedding_dim], then self.out(emb) produces logits over the whole vocabulary [batch_size, vocab_size] to predict a context word from the center word.\n",
    "\n",
    "2. CBOW (CBOW):\n",
    "    - Input: multiple context word indices per example, shape [batch_size, context_len].\n",
    "    - Forward: self.embeddings(inputs) returns [batch_size, context_len, dims], then mean(dim=1) aggregates all context word embeddings into one vector [batch_size, dims], and self.linear(embeds) outputs [batch_size, vocab_size] to predict the center (target) word.\n",
    "\n",
    "- In sum, CBOW does many context words -> average embedding -> predict one target, while Skip-gram does one target word -> embedding -> predict a context word. They have different input shapes and the mean(dim=1) aggregation step only appears in the CBOW forward."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
